{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "building decision tree with training set\n",
    "\n",
    "Drug <A>\n",
    "\n",
    "Drug <B>\n",
    "\n",
    "Decision Tree:\n",
    "- age\n",
    "    - young\n",
    "        - sex\n",
    "            - F <A>\n",
    "            - M <B>\n",
    "    - middle-age <B>\n",
    "\n",
    "    - senior\n",
    "        - cholesterol\n",
    "            - high <A>\n",
    "            - normal <B>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- each \"internal node\" corresponds to a test.\n",
    "- and each \"branch\" corresponds to a result of the test.\n",
    "- and each \"leaf node\" assigns a patient to a class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Choose an attribute from dataset\n",
    "2. Calculate the significance of the attribute in the splitting of the data.\n",
    "3. Split the data based on the value of the best attribute\n",
    "4. Go to step 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- more predictiveness\n",
    "- less impurity\n",
    "- lower entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Pure Node\" : A node in the tree is considered pure if, in all of the cases the nodes fall into a specific category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy \n",
    "Information disorder or randomnes in the data. \n",
    "The Entropy is used to calculate the homogeneity of the samples in that node.\n",
    "- #### to do check entropy formula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tree with the higher information gain after splitting.\n",
    "\n",
    "Information gain = entropy before split - weighted entropy after split"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
